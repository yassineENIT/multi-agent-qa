import fitz  # PyMuPDF
import openai
import spacy
import time
import networkx as nx
from sentence_transformers import SentenceTransformer, util

# Configuration des modÃ¨les et clÃ©s API
openai.api_key = "your-api-key-here"
embedder = SentenceTransformer("sentence-transformers/multi-qa-mpnet-base-dot-v1")

# ğŸ“„ Ã‰tape 1 : Extraire le texte dâ€™un PDF
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# âœ‚ï¸ Ã‰tape 2 : Chunker le texte en petits morceaux (Ã©vite dÃ©passement de tokens)
def chunk_text(text, max_chars=800):  # max_chars Ã©quilibrÃ© pour Ã©viter les erreurs
    paragraphs = text.split("\n\n")
    chunks = []
    current = ""
    for para in paragraphs:
        if len(current) + len(para) < max_chars:
            current += para + "\n\n"
        else:
            chunks.append(current.strip())
            current = para + "\n\n"
    if current:
        chunks.append(current.strip())
    return chunks

# ğŸ¤– Ã‰tape 3 : RÃ©sumer chaque chunk avec GPT-3.5
def summarize_chunk(chunk):
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You summarize academic content clearly in 2-3 bullet points."},
                {"role": "user", "content": f"Summarize the following in 2-3 key ideas:\n{chunk}"}
            ],
            max_tokens=200,
            temperature=0.3
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        print(f"âŒ Erreur lors du rÃ©sumÃ© : {e}")
        return "[RÃ©sumÃ© indisponible pour ce chunk]"

# ğŸ§  Ã‰tape 4 : Extraire les entitÃ©s et les relations
def extract_entities_and_edges(summary, graph):
    doc = nlp(summary)
    entities = [ent.text for ent in doc.ents]
    for i in range(len(entities)):
        for j in range(i + 1, len(entities)):
            graph.add_edge(entities[i], entities[j], label="related")

# ğŸ” Ã‰tape 5 : Recherche par similaritÃ© dans les rÃ©sumÃ©s
def search_mind_map(summaries, query):
    query_embedding = embedder.encode(query, convert_to_tensor=True)
    best_score = 0
    best_summary = None
    for summary in summaries:
        summary_embedding = embedder.encode(summary, convert_to_tensor=True)
        score = util.cos_sim(query_embedding, summary_embedding).item()
        if score > best_score:
            best_score = score
            best_summary = summary
    print(f"\nğŸ” Question : '{query}'")
    print(f"âœ… RÃ©sumÃ© pertinent (score {best_score:.2f}):\n{best_summary}")

# ğŸš€ Programme principal
def main(pdf_path):
    print("ğŸ“„ Lecture du fichier PDF...")
    pdf_text = extract_text_from_pdf(pdf_path)

    print("âœ‚ï¸ DÃ©coupage du texte...")
    chunks = chunk_text(pdf_text, max_chars=800)  # AjustÃ© pour GPT-3.5

    mind_map_graph = nx.Graph()
    summaries = []

    print("ğŸ¤– RÃ©sumÃ© des chunks avec GPT-3.5...")
    for i, chunk in enumerate(chunks[:30]):  # Facultatif : limite les chunks analysÃ©s
        print(f"ğŸ”¹ Chunk {i+1}/{len(chunks)}...")
        summary = summarize_chunk(chunk)
        summaries.append(summary)
        extract_entities_and_edges(summary, mind_map_graph)
        time.sleep(1.2)  # Ã‰vite de dÃ©passer les limites dâ€™API

    print("\nğŸ§  RÃ©sumÃ©s extraits :")
    for idx, s in enumerate(summaries):
        print(f"\nğŸ”¹ RÃ©sumÃ© {idx+1}:\n{s}")

    # ğŸ” Recherche dans les rÃ©sumÃ©s
    search_mind_map(summaries, "How does Weibull distribution apply to reliability engineering?")

# ğŸ”§ ExÃ©cute le programme
if __name__ == "__main__":
    main("file.pdf")
